# 爬蟲講義

這裡的`index.html`是我原本看的影片，但講義後面的影片就沒了，可能要付費之類的，總之就是覺得丟掉挺可惜的就把他留著。  
因為影片後面沒了，就把學到和剩下的部分作整理  
scrapy的部分我感覺有點複雜+影片也沒講就只有大概理解一下，我發現除非需要爬取大量資料，不然使用requests+bs4就夠了。

## 基本爬取資料的方式

### 快速總結

| 特性         | 正則表達式（Regex）          | BeautifulSoup（BS4）  | XPath                      |
|------------|-----------------------|---------------------|----------------------------|
| **定位方法**   | 通過模式匹配字串              | 標籤、屬性               | 節點層級關係                     |
| **靈活性**    | 高，匹配模式可以任意設計          | 中等，依賴標籤和屬性          | 高，支持複雜的條件和結構化數據處理          |
| **適用範圍**   | 無結構或半結構文本             | HTML/XML 結構化內容      | 結構化程度高的數據（XML/HTML）        |
| **學習成本**   | 中等，語法需花時間熟悉           | 低，API 友好            | 高，語法複雜                     |
| **性能**     | 高效，純文本處理速度快           | 較慢，處理大文件性能下降        | 高效，特別是處理大規模結構化數據           |
| **動態內容支持** | 無法處理，需結合其他工具          | 無法處理，需結合 Selenium 等 | 無法處理，需結合 Selenium 或 Splash |
| **典型使用場景** | 電子郵件、電話號碼、URL 提取；日誌分析 | 快速提取靜態網頁數據          | 精準提取 XML/HTML 數據           |

### 選擇建議

1. **選擇正則表達式（Regex）**：
    - 如果需要快速處理無結構或半結構文本（如提取郵箱、清洗數據）。
    - 適合處理格式固定的小型數據任務。

2. **選擇 BeautifulSoup（BS4）**：
    - 適用於靜態 HTML 網頁解析。
    - 如果目標數據基於標籤和屬性定位，BS4 是首選，易於使用和擴展。

3. **選擇 XPath**：
    - 當處理結構化程度高的 HTML/XML 時，XPath 能提供精確的查詢能力。
    - 適合大規模、複雜結構的數據提取，尤其是在用戶熟悉 XPath 語法的情況下。

---

### 正則(regex)

#### 功能

- 正則表達式是一種匹配字串模式的語法，用於搜索、替換或分割文本。
- 用於基於模式匹配對純文本進行操作。

#### 使用場景

- **精確匹配**：當數據具有固定格式（如郵箱、電話號碼、日期）時使用。
- **文本處理**：從網頁、日誌文件或其他文本源中提取關鍵數據。
- **替換操作**：清洗數據，將匹配的模式替換成新的字符串。
- **無結構數據**：處理難以使用結構化工具解析的內容。

#### 適用範例

```python
import re

# 提取所有郵箱地址
text = "Contact us at support@example.com or admin@website.com."
emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', text)
print(emails)  # ['support@example.com', 'admin@website.com']
```

#### 優勢

- 精準控制模式。
- 對純文本的操作極為高效。
- 跨語言支持，通用性高。

#### 缺點

- **難以讀懂**：正則表達式的語法對新手不友好。
- **結構化限制**：對於 HTML 樹狀結構的處理很弱。
- **易出錯**：若 HTML 或文本格式稍有變化，模式可能失效。

### xpath

#### 功能

- XPath 是一種在 XML 或 HTML 文件中導航樹狀結構的語言，用於定位節點和提取數據。
- 基於節點的層級關係，支持條件過濾和模式匹配。

#### 使用場景

- **高度結構化內容**：如 XML 或 HTML，特別是表格、層級分明的數據。
- **多屬性過濾**：需要精確查詢特定屬性或標籤的元素。
- **高效定位**：當 HTML 結構複雜且需要精準選取目標內容時。

#### 適用範例

```python
from lxml import etree

html = """
<html>
    <body>
        <div>
            <h1>Title</h1>
            <p class="description">This is a description.</p>
        </div>
    </body>
</html>
"""

tree = etree.HTML(html)
title = tree.xpath("//h1/text()")[0]
description = tree.xpath("//p[@class='description']/text()")[0]
print(title)  # Title
print(description)  # This is a description.
```

#### 優勢

- **精確查詢**：支持層級過濾、屬性篩選等操作。
- **性能高效**：專為結構化數據設計，解析速度快。
- **靈活性強**：可使用各種查詢方式（絕對路徑、相對路徑、條件篩選）。

#### 缺點

- **語法較難**：對新手不友好，學習曲線較高。
- **依賴結構**：對 HTML/XML 結構敏感，結構變動時需修改路徑。
- 需要特定的解析庫（如 lxml）。

### BeautifulSoup(bs4)

#### 功能

- 提供一套友好的 Python API 用於解析和提取 HTML/XML 的數據。
- 支持解析器（如 lxml、html.parser）用於理解 HTML 結構。
- 主要通過標籤和屬性來定位和提取數據。

#### 使用場景

- **HTML/XML文件解析**： 快速抓取靜態網頁中的內容。
- **結構化數據提取**： 基於標籤（Tag）屬性（Attributes）提取數據，如標題、表格、列表。
- **清洗和整理數據**： 可以輕鬆過濾掉不必要的內容。

#### 適用範例

```python
from bs4 import BeautifulSoup

html = """
<html>
    <body>
        <h1>Title</h1>
        <p class="description">This is a description.</p>
    </body>
</html>
"""

soup = BeautifulSoup(html, "html.parser")
title = soup.find("h1").text
description = soup.find("p", class_="description").text
print(title)  # Title
print(description)  # This is a description.
```

#### 優勢

- **易學易用**：API 簡單，適合快速開發。
- **結構化處理**：專注於標籤和屬性，能輕鬆定位和提取內容。
- 支持多種解析器（如 `html.parser`, `lxml`）。

#### 缺點

- **效率較低**：在處理大規模 HTML 文件時，性能可能不如專門的解析工具。
- **僅限靜態內容**：無法直接解析 JavaScript 動態生成的數據。
- **依賴 HTML 結構**：結構變化可能導致程式需要調整。

## 爬取的框架

### 快速總結

| 特性                 | Requests                            | Selenium                            | Scrapy                              |
|----------------------|-------------------------------------|-------------------------------------|-------------------------------------|
| **主要功能**          | 發送 HTTP 請求並獲取靜態內容         | 瀏覽器模擬與動態內容提取             | 高性能爬取與數據處理框架             |
| **適用場景**          | 靜態網站、API 數據抓取              | 動態網站、模擬用戶交互               | 大規模結構化爬取、定期抓取           |
| **優勢**             | 簡單高效，適合 API 和靜態頁面        | 支持動態內容和用戶交互               | 支持異步高並發爬取，內置數據管道      |
| **劣勢**             | 無法處理 JavaScript 動態內容         | 性能較低，配置和資源消耗大            | 學習曲線陡峭，對小型任務過於複雜     |
| **性能**             | 高效（適合 API 和靜態內容）          | 慢（需要模擬瀏覽器）                 | 高效（異步處理，高並發）             |
| **學習曲線**          | 低，適合新手快速上手                | 中等，需了解瀏覽器模擬                | 高，需要理解框架結構與設計            |
| **動態內容支持**      | 無法處理                           | 完全支持                           | 需要額外集成 Splash 或 Selenium     |
| **並發性能**          | 低（需要手動實現並發）              | 低（每個瀏覽器進程獨立）             | 高（內置異步支持）                   |
| **典型使用工具**      | 與 BeautifulSoup 結合              | 與動態交互腳本結合                   | 與 Redis、MongoDB 集成               |

---

### 選擇建議

1. **選擇 Requests**  
   - 當需要快速抓取靜態數據或與 API 通信時。
   - 適合初學者以及輕量級爬取需求。

2. **選擇 Selenium**  
   - 當網站需要 JavaScript 渲染或模擬複雜用戶操作時。
   - 適合小規模、動態內容抓取。

3. **選擇 Scrapy**  
   - 當需要高效、結構化的數據抓取，且爬取規模較大時。
   - 適合專業爬蟲開發和需要長期運行的抓取任務。

---

### Requests

#### 功能

- **Requests** 是一個 Python 的 HTTP 庫，用於與網頁進行 HTTP 請求交互（GET、POST 等）。
- 專注於發送 HTTP 請求並獲取靜態 HTML 和 JSON 等內容。

#### 使用場景

- **靜態網站爬取**：適用於內容靜態、數據直接嵌入 HTML 的網站。
- **API 數據交互**：與 REST API 通信（例如獲取 JSON 數據）。
- **模擬 HTTP 請求**：模擬瀏覽器行為，如添加 Headers 或 Cookies。
- **簡單下載器**：用於下載文件、圖片等。

#### 優點

- **簡單易用**：API 設計直觀，能快速發送請求。
- **性能高效**：專注於請求/響應處理，適合大量簡單請求。
- **靈活性強**：可自定義 Headers、Cookie、User-Agent，模擬各種瀏覽器。

#### 缺點

- **無法處理動態內容**：無法執行 JavaScript，對於 SPA（單頁應用）或 AJAX 驅動的網站無效。
- **需要結合其他工具**：無法直接解析 HTML，需要結合 BeautifulSoup 或 lxml 來處理內容。

#### 範例

```python
import requests

# 發送 GET 請求
response = requests.get("https://example.com")
if response.status_code == 200:
    print(response.text)  # 打印 HTML 內容
```

### Selenium

#### 功能

- **Selenium** 是一個瀏覽器自動化工具，可以模擬用戶操作（點擊、輸入、滾動等）並抓取動態生成的網頁內容。
- 通過驅動瀏覽器執行 JavaScript，解析頁面內容。

#### 使用場景

- **動態網站抓取**：如 JavaScript 驅動的 SPA 和 AJAX 動態加載的網站。
- **驗證碼交互**：需要模擬用戶操作（例如手動輸入驗證碼）。
- **測試網頁交互行為**：如表單提交、按鈕點擊、滾動。
- **捕獲動態內容**：如頁面載入後生成的數據。

#### 優點

- **動態內容支持**：能處理 JavaScript 動態渲染的頁面。
- **模擬真實用戶行為**：模擬點擊、輸入、鼠標操作等。
- **多瀏覽器支持**：支持 Chrome、Firefox、Edge 等。

#### 缺點

- **性能較差**：啟動瀏覽器的開銷大，適合小規模抓取。
- **資源消耗高**：對於大規模抓取，資源使用量較高。
- **配置麻煩**：需要安裝瀏覽器驅動和相關依賴。

#### 範例

```python
from selenium import webdriver

driver = webdriver.Chrome()
driver.get("https://example.com")

# 獲取動態生成的數據
title = driver.title
print(title)

driver.quit()
```

### Scrapy

#### 功能

- **Scrapy** 是一個專業的爬蟲框架，提供爬取、解析和數據存儲的一站式解決方案。
- 支持分佈式爬取、異步請求和管道化數據處理。

#### 使用場景

- **大型網站抓取**：需要定期爬取、更新或抓取多頁數據的網站。
- **高性能抓取**：需要快速處理大量頁面的數據提取。
- **數據存儲處理**：數據需要直接存入數據庫或進行進一步處理。

#### 優點

- **高效**：基於異步框架（Twisted），支持高並發請求。
- **擴展性強**：支持自定義中間件、擴展插件等。
- **內置功能豐富**：包括爬取器、解析器、數據管道、下載中間件等。
- **分佈式支持**：結合 Scrapy-Redis 等工具能實現分佈式爬蟲。

#### 缺點

- **學習曲線高**：框架設計複雜，對新手不友好。
- **動態內容支持有限**：需要與 Selenium 或 Splash 結合處理動態頁面。
- **適合結構化抓取**：對於小型、臨時性爬取需求顯得過於笨重。

#### 範例

```python
import scrapy

class ExampleSpider(scrapy.Spider):
    name = "example"
    start_urls = ["https://example.com"]

    def parse(self, response):
        title = response.xpath("//title/text()").get()
        print(title)
```
